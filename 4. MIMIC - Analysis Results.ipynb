{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the models obtain previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lifelines.fitters.kaplan_meier_fitter import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "import seaborn as sns\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False, \"axes.spines.left\": False,\n",
    "                 \"axes.spines.bottom\": False, \"figure.dpi\": 300, 'savefig.dpi': 300}\n",
    "sns.set_theme(style = \"whitegrid\", rc = custom_params, font_scale = 1.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100\n",
    "\n",
    "subselection = True\n",
    "mode = \"admission\"\n",
    "\n",
    "subsample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_csv('data/mimic/outcomes_first_day{}.csv'.format('_subselection' if subselection else ''), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'results{}/mimic/'.format('_subselection' if subselection else '')\n",
    "\n",
    "results_weekend = path + 'weekends/' # Train only on weekends but test on both\n",
    "results_weekdays  = path + 'weekdays/' # Train only on weekdays but test on both\n",
    "periods = [\"Weekend\", \"Weekday\"]\n",
    "\n",
    "# Random split\n",
    "results_random  = 'results{}/mimic/random/'.format('_subselection' if subselection else '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for period, results in zip(periods + ['Random'], [results_weekend, results_weekdays, results_random]):\n",
    "    predictions[period] = {}\n",
    "    root = 'survival_'\n",
    "    print(period)\n",
    "    for file in sorted(os.listdir(results)):\n",
    "        if (root not in file) or ('.csv' not in file):\n",
    "            continue\n",
    "\n",
    "        if (period == 'Weekday') and subsample:\n",
    "            if not('under' in file):\n",
    "                continue\n",
    "        elif (period == 'Weekday') and ('under' in file):\n",
    "            continue   \n",
    "        \n",
    "        name = file[file.index(root)+len(root):file.rindex('.csv')]\n",
    "        predictions[period][name] = pd.read_csv(results + file, index_col=0)\n",
    "        print(file, ' -> ', name)\n",
    "\n",
    "# Select only if present in both\n",
    "intersection = predictions[periods[0]].keys() & predictions[periods[1]].keys() & predictions['Random'].keys()\n",
    "labels = {}\n",
    "for period in periods:\n",
    "    predictions[period] = {model: predictions[period][model] for model in intersection}\n",
    "    labels[period] = predictions[period][list(intersection)[0]].Use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaplman Meier estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [7, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = labels[periods[0]] != 'External'\n",
    "kmf = KaplanMeierFitter(label = periods[0])\n",
    "kmf.fit(outcomes.Remaining.loc[test], outcomes.Event.loc[test])\n",
    "kmf.plot()\n",
    "\n",
    "test = labels[periods[1]] != 'External'\n",
    "kmf = KaplanMeierFitter(label = periods[1])\n",
    "kmf.fit(outcomes.Remaining.loc[test], outcomes.Event.loc[test])\n",
    "kmf.plot()\n",
    "\n",
    "for h in horizons:\n",
    "    plt.axvline(h, ls = '--') \n",
    "\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.xlabel('Time after observation (in days)')\n",
    "plt.ylabel('Survival estimation')\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0.8, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencesin observed labels between training and testing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all metrics on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute and display performances per group of model\n",
    "models_group = {\"Baselines\": [\"deepsurv\"],\n",
    "                \"Sequential\": [\"lstm\"],\n",
    "                \"Time aware\": [\"gru\"],\n",
    "                \"Proposed\": [\"joint\"]}\n",
    "\n",
    "performances, rocs = {}, {}\n",
    "for source in labels:\n",
    "    performances[source], rocs[source] = {}, {}\n",
    "    for target in labels:\n",
    "        print('-' * 42)\n",
    "        print('{} -> {}'.format(source, target))\n",
    "\n",
    "        performances[source][target], rocs[source][target] = {}, {}\n",
    "        for group in models_group:\n",
    "            print('*' * 21)\n",
    "            print(group)\n",
    "            \n",
    "            for model in sorted(predictions[source].keys()):\n",
    "                if not(any([m in model for m in models_group[group]])):\n",
    "                    continue\n",
    "                np.random.seed(42)\n",
    "                preds = predictions[source][model]\n",
    "\n",
    "                print(model)\n",
    "                # Target to ensure fair comparison !!!\n",
    "                selection = outcomes.Day <= 4 if (target == 'Weekday') else outcomes.Day > 4 # Update to use all data even when under sampling\n",
    "                test = (labels[target] != 'Train') & (selection) # Use the data that will be used for both (you want to use the subset of point that was not used for test in the other group)\n",
    "                test = test[test].index\n",
    "\n",
    "                train = labels[target] == 'Train' # Use Kaplan meier on the training data of the target (you want to use the target in the other group)\n",
    "                train = train[train].index\n",
    "\n",
    "                performances[source][target][model], rocs[source][target][model] = evaluate(outcomes.Event, outcomes.Remaining, preds.drop(columns = 'Use'), train, test, horizons = horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances['Random'], rocs['Random'] = {'Random': {}}, {'Random': {}}\n",
    "for model in sorted(predictions['Random'].keys()):\n",
    "    np.random.seed(42)\n",
    "    preds = predictions['Random'][model]\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    test = preds.Use != 'Train'\n",
    "    test = test[test].index\n",
    "\n",
    "    train = preds.Use == 'Train'\n",
    "    train = train[train].index\n",
    "\n",
    "    performances['Random']['Random'][model], rocs['Random']['Random'][model] = evaluate(outcomes.Event, outcomes.Remaining, preds.drop(columns = 'Use'), train, test, horizons = horizons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons_display = ['7', '30', 'Overall']\n",
    "plot = \"TD Concordance Index\" #\"Brier Score\", \"TD Concordance Index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming = {\n",
    "    'joint_value+time+mask': r\"$\\bf{DeepJoint}$\",\n",
    "\n",
    "    # 'joint+missing_value+time+mask': r\"$\\bf{DeepJoint - Missing}$\",\n",
    "    # 'joint+time_value+time+mask': r\"$\\bf{DeepJoint - Time}$\",\n",
    "    'lstm_value+time+mask': \"Feature\",\n",
    "\n",
    "    'gru_d': \"GRU-D\",\n",
    "\n",
    "    'lstm_value': \"Ignore\",\n",
    "    'lstm+resampled': \"Resample\",\n",
    "    'deepsurv_count': \"Count\",\n",
    "    'deepsurv_last': \"Last\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_display = {\n",
    "    s :{\n",
    "        t: {\n",
    "            name: performances[s][t][model][plot]\n",
    "            for model, name in naming.items()\n",
    "        }\n",
    "        for t in performances[s]\n",
    "    }\n",
    "    for s in periods + ['Random']\n",
    "}\n",
    "rocs_display = {\n",
    "    s :{\n",
    "        t: {\n",
    "            name: {str(i): np.array(rocs[s][t][model][i]) for i in rocs[s][t][model]}\n",
    "            for model, name in naming.items()\n",
    "        }\n",
    "        for t in performances[s]\n",
    "    }\n",
    "    for s in periods + ['Random']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(plt.rcParams['axes.prop_cycle'])[:len(naming)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdst = pd.concat(performances_display['Random']['Random'], axis = 1).iloc[:,::-1]\n",
    "\n",
    "#plt.rcParams.update({'font.size': 12})\n",
    "fig, axes = plt.subplots(ncols = len(horizons_display), sharey = True, sharex = True, figsize=(12,5))\n",
    "print(\"{} -> {}\".format('Random', 'Random'))\n",
    "for i, ax in zip(horizons_display, axes):\n",
    "    perf_metric_mean = pdst.loc['Mean', i]\n",
    "    perf_metric_std = 1.96 * pdst.loc['Std', i] / np.sqrt(iters)\n",
    "    for j, (model, c) in enumerate(zip(pdst.columns, colors[::-1])):\n",
    "        p = ax.plot((perf_metric_mean[model] + perf_metric_std[model], perf_metric_mean[model] - perf_metric_std[model]), (j, j), c = c['color'], alpha = 0.5, linewidth = 4)\n",
    "        ax.scatter(perf_metric_mean[model], j, s = 200, label = model, marker = '|', color = p[-1].get_color(),linewidths = 4)\n",
    "        \n",
    "    ax.grid(alpha = 0.3)\n",
    "    ax.set_yticks(range(len(pdst.columns)))\n",
    "    ax.set_yticklabels(pdst.columns)\n",
    "    ax.set_title('{} days'.format(i) if i != 'Overall' else 'Integrated')\n",
    "    #ax.set_xlabel(plot)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(pd.DataFrame.from_dict({m: [\"{:.3f} ({:.3f})\".format(pdst.loc['Mean'].loc[m].loc[i], pdst.loc['Std'].loc[m].loc[i]) for i in pdst.loc['Mean'].columns] for m in horizons_display}, columns = pdst.columns, orient = 'index').T.loc[::-1].to_latex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between weekend and weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot double barh to display performances\n",
    "### Create dfs of mean and std\n",
    "for time in performances_display:\n",
    "    opposite = periods[1] if time == periods[0] else periods[0]\n",
    "\n",
    "    transfer = \"{} -> {}\".format(opposite, time)\n",
    "    training = \"{} -> {}\".format(time, time)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols = len(horizons_display), figsize = (14, 3))\n",
    "    \n",
    "    axes[0].set_ylabel(transfer)\n",
    "\n",
    "    for ax, horizon, m in zip(axes, horizons_display, ['o', 'D', 'x', '.']):\n",
    "        perf_metric_mean = pd.DataFrame({\n",
    "                    transfer : pd.concat(performances_display[opposite][time], axis = 1).loc['Mean', horizon],\n",
    "                    training : pd.concat(performances_display[time][time], axis = 1).loc['Mean', horizon]\n",
    "                })\n",
    "        perf_metric_std =  pd.DataFrame({\n",
    "                    transfer : pd.concat(performances_display[opposite][time], axis = 1).loc['Std', horizon],\n",
    "                    training : pd.concat(performances_display[time][time], axis = 1).loc['Std', horizon]\n",
    "                }) \n",
    "\n",
    "        for model, c in zip(perf_metric_mean.index, colors[:len(perf_metric_mean)]):\n",
    "            ax.scatter(perf_metric_mean.loc[model][training], perf_metric_mean.loc[model][transfer], color = c['color'], marker = m, alpha = 0.5, s = 100)\n",
    "            ax.plot([perf_metric_mean.loc[model][training] - 1.96 *perf_metric_std.loc[model][training]/ np.sqrt(iters), perf_metric_mean.loc[model][training] + 1.96 *perf_metric_std.loc[model][training]/ np.sqrt(iters)], [perf_metric_mean.loc[model][transfer], perf_metric_mean.loc[model][transfer]], color = c['color'])\n",
    "            ax.plot([perf_metric_mean.loc[model][training], perf_metric_mean.loc[model][training]], [perf_metric_mean.loc[model][transfer] - 1.96*perf_metric_std.loc[model][transfer]/ np.sqrt(iters), perf_metric_mean.loc[model][transfer] + 1.96* perf_metric_std.loc[model][transfer]/ np.sqrt(iters)], color = c['color'])\n",
    "\n",
    "        ax.axline((perf_metric_mean.mean().mean(), perf_metric_mean.mean().mean()), slope=1, color = 'k', ls = ':', alpha = 0.5)\n",
    "        ax.set_xlabel(training)\n",
    "        ax.grid(alpha = 0.5)\n",
    "\n",
    "        means = perf_metric_mean.mean()\n",
    "        margin = 1.96 * perf_metric_mean.std().max()\n",
    "        ax.set_xlim(means[training] - margin, means[training] + margin)\n",
    "        ax.set_ylim(means[transfer] - margin, means[transfer] + margin)\n",
    "        ax.set_title('At {} days'.format(horizon) if horizon != 'Overall' else 'Integrated')\n",
    "\n",
    "        table = {\n",
    "            mode: {\n",
    "                model: \"{:.3f} ({:.3f})\".format(perf_metric_mean.loc[model][mode], perf_metric_std.loc[model][mode]) for model in perf_metric_mean.index\n",
    "            } for mode in perf_metric_mean.loc[model].index\n",
    "        }\n",
    "        diff = {name: np.abs(rocs_display[time][time][name][horizon] - rocs_display[opposite][time][name][horizon]) for model, name in naming.items()}\n",
    "        table['Difference'] = {model: \"{:.3f} ({:.3f})\".format(diff[model].mean(), diff[model].std()) for model in diff}\n",
    "        table = pd.DataFrame(table)\n",
    "        print(horizon, table.to_latex())\n",
    "    else:\n",
    "        # Display\n",
    "        ## Legend\n",
    "        for model, c in zip(perf_metric_mean.index, colors[:len(perf_metric_mean)]):\n",
    "            plt.scatter([],[], color = c['color'], label = model)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), title = 'Models')\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgroup Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the difference in change for different group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = outcomes.GENDER == 'F' #outcomes.Sexoutcomes.INSURANCE.replace({\"Medicare\": \"Public\", \"Medicaid\": \"Public\", \"Government\": \"Public\"}) != 'Public'\n",
    "groups = ['Male', 'Female']#['Private', 'Public']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs_group = {}\n",
    "for group, subgroups in zip(groups, [subgroups, ~subgroups]):\n",
    "    rocs_group[group] = {}\n",
    "    for source in labels:\n",
    "        rocs_group[group][source] = {}\n",
    "        for target in labels:\n",
    "            print('-' * 42)\n",
    "            print('{} -> {}'.format(source, target))\n",
    "            rocs_group[group][source][target] = {}\n",
    "\n",
    "            for family in models_group:\n",
    "                print('*' * 21)\n",
    "                print(family)\n",
    "                \n",
    "                for model in sorted(predictions[source].keys()):\n",
    "                    if not(any([m in model for m in models_group[family]])):\n",
    "                        continue\n",
    "                    np.random.seed(42)\n",
    "                    preds = predictions[source][model].loc[subgroups]\n",
    "\n",
    "                    print(model)\n",
    "                    # Target to ensure fair comparison !!!\n",
    "                    selection = outcomes.Day <= 4 if (target == 'Weekday') else outcomes.Day > 4 # Update to use all data even when under sampling\n",
    "                    test = (labels[target].loc[subgroups] != 'Train') & (selection.loc[subgroups]) # Use the data that will be used for both (you want to use the subset of point that was not used for test in the other group)\n",
    "                    test = test[test].index\n",
    "\n",
    "                    train = labels[target].loc[subgroups] == 'Train' # Use Kaplan meier on the training data of the target (you want to use the target in the other group)\n",
    "                    train = train[train].index\n",
    "\n",
    "                    _, rocs_group[group][source][target][model] = evaluate(outcomes.Event, outcomes.Remaining, preds.drop(columns = 'Use'), train, test, horizons = horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs_display = {\n",
    "    group: {\n",
    "        s :{\n",
    "            t: {\n",
    "                name: {str(i): np.array(rocs_group[group][s][t][model][i]) for i in rocs_group[group][s][t][model]}\n",
    "                for model, name in naming.items()\n",
    "            }\n",
    "            for t in performances[s]\n",
    "        }\n",
    "        for s in periods\n",
    "    }\n",
    "    for group in groups\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = {}\n",
    "for time in performances_display:\n",
    "    opposite = periods[1] if (time == periods[0]) else periods[0]\n",
    "    difference[time], display = {}, {}\n",
    "    for group in groups:\n",
    "        difference[time][group] = {name: pd.Series(rocs_display[group][time][time][name]['Overall'] - rocs_display[group][opposite][time][name]['Overall']).abs() for model, name in naming.items()}\n",
    "        delta_text = {model: \"{:.3f} ({:.3f})\".format(difference[time][group][model].mean(), difference[time][group][model].std()) for model in difference[time][group]}\n",
    "        display[group] = pd.Series(delta_text)\n",
    "    difference[time] = {name: difference[time][groups[0]][name] - difference[time][groups[1]][name] for model, name in naming.items()}\n",
    "    display['Difference'] = pd.Series({model: \"{:.3f} ({:.3f})\".format(difference[time][model].mean(), difference[time][model].std()) for model in difference[time]})\n",
    "    display = pd.concat(display, axis = 1)\n",
    "    print(time, horizon, display.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in performances_display:\n",
    "    fig, axes = plt.subplots(ncols = 2, figsize = (9, 3))\n",
    "    opposite = periods[1] if time == periods[0] else periods[0]\n",
    "\n",
    "    transfer = \"{} -> {}\".format(opposite, time)\n",
    "    training = \"{} -> {}\".format(time, time)\n",
    "\n",
    "    axes[0].set_ylabel(transfer)\n",
    "\n",
    "    for group, ax in zip(groups, axes):\n",
    "        perf_metric_mean = pd.DataFrame({\n",
    "                    transfer : pd.Series({model: rocs_display[group][opposite][time][model]['Overall'].mean() for model in rocs_display[group][opposite][time]}),\n",
    "                    training : pd.Series({model: rocs_display[group][time][time][model]['Overall'].mean() for model in rocs_display[group][opposite][time]}),\n",
    "                })\n",
    "        perf_metric_std =  pd.DataFrame({\n",
    "                    transfer : pd.Series({model: rocs_display[group][opposite][time][model]['Overall'].std() for model in rocs_display[group][opposite][time]}),\n",
    "                    training : pd.Series({model: rocs_display[group][time][time][model]['Overall'].std() for model in rocs_display[group][opposite][time]}),\n",
    "\n",
    "                }) \n",
    "        colors = list(plt.rcParams['axes.prop_cycle'])\n",
    "        #colors[3] = colors[8]\n",
    "        for model, c in zip(perf_metric_mean.index, colors[:len(perf_metric_mean)]):\n",
    "            ax.scatter(perf_metric_mean.loc[model][training], perf_metric_mean.loc[model][transfer], color = c['color'], marker = '.', alpha = 0.5, s = 100)\n",
    "            ax.plot([perf_metric_mean.loc[model][training] - 1.96 *perf_metric_std.loc[model][training]/ np.sqrt(iters), perf_metric_mean.loc[model][training] + 1.96 *perf_metric_std.loc[model][training]/ np.sqrt(iters)], [perf_metric_mean.loc[model][transfer], perf_metric_mean.loc[model][transfer]], color = c['color'])\n",
    "            ax.plot([perf_metric_mean.loc[model][training], perf_metric_mean.loc[model][training]], [perf_metric_mean.loc[model][transfer] - 1.96*perf_metric_std.loc[model][transfer]/ np.sqrt(iters), perf_metric_mean.loc[model][transfer] + 1.96* perf_metric_std.loc[model][transfer]/ np.sqrt(iters)], color = c['color'])\n",
    "\n",
    "        ax.axline((perf_metric_mean.mean().mean(), perf_metric_mean.mean().mean()), slope=1, color = 'k', ls = ':', alpha = 0.5)\n",
    "        ax.set_xlabel(training)\n",
    "        ax.grid(alpha = 0.5)\n",
    "\n",
    "        means = perf_metric_mean.mean()\n",
    "        margin = 1.96 * perf_metric_mean.std().max()\n",
    "        ax.set_xlim(means[training] - margin, means[training] + margin)\n",
    "        ax.set_ylim(means[transfer] - margin, means[transfer] + margin)\n",
    "        ax.set_title(group)\n",
    "    else:\n",
    "        # Display\n",
    "        ## Legend\n",
    "        for model, c in zip(perf_metric_mean.index, colors[:len(perf_metric_mean)]):\n",
    "            plt.scatter([],[], color = c['color'], label = model)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), title = 'Models')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('survival')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
